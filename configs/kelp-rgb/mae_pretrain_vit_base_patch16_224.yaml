seed_everything: 42

data:
  class_path: "src.data.MAEDataModule"
  init_args:
    train_chip_dir: "/home/taylor/data/KS-RGB-Jul2025-224-clean/train"
    val_chip_dir: "/home/taylor/data/KS-RGB-Jul2025-224-clean/val"
    test_chip_dir: "/home/taylor/data/KS-RGB-Jul2025-224-clean/test"
    batch_size: 256
    fill_value: 0
    fill_mask: 0
    num_workers: 8
    pin_memory: true
    persistent_workers: true

model:
  class_path: "src.models.mae_pretrain.MAE"
  init_args:
    mask_ratio: 0.7
    model_name: "vit_base_patch16_224"
    lr: 1e-4
    wd: 0.05
    b1: 0.9
    b2: 0.999

trainer:
  accelerator: auto
  devices: auto
  num_nodes: 1
  precision: bf16-mixed
  log_every_n_steps: 5
  max_epochs: 100
#  accumulate_grad_batches: 2
#  gradient_clip_val: 0.8
  detect_anomaly: false
  default_root_dir: checkpoints/kom-kelp-rgb
  fast_dev_run: False
  logger:
    - class_path: lightning.pytorch.loggers.WandbLogger
      init_args:
        entity: hakai
        project: kom-mae-pretrain
        group: Jul2025
        log_model: true
        tags:
          - kelp
          - Jul2025
  callbacks:
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        dirpath: checkpoints/kom-kelp-rgb
        auto_insert_metric_name: False
        filename: mae_pretrain_epoch-{epoch:02d}_loss-{val/loss_epoch:.4f}
        monitor: val/loss_epoch
        mode: min
        save_last: True
        save_top_k: 2
        save_weights_only: True
        verbose: True
    - class_path: lightning.pytorch.callbacks.LearningRateMonitor
      init_args:
        logging_interval: step
  plugins:
    - class_path: lightning.pytorch.plugins.io.AsyncCheckpointIO
