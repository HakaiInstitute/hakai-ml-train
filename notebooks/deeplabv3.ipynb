{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "import os\n",
    "from tqdm.auto import tqdm, trange\n",
    "from collections import OrderedDict\n",
    "from PIL import Image\n",
    "from torchvision.transforms import ToPILImage\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "from utils.dataset import SegmentationDataset, TransformDataset, transforms as T\n",
    "from utils.vis import show_torch_batch, show_torch_img\n",
    "from utils.metrics import ConfusionMatrix\n",
    "from models import deeplabv3, half_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "disable_cuda = False\n",
    "num_classes = 2\n",
    "num_epochs = 30\n",
    "batch_size = 4\n",
    "ignore_index = 100  # Value of labels that we ignore in loss and other logic (e.g. kelp with unknown species)\n",
    "prep_datasets = True\n",
    "dataset_path = Path('../checkpoints/datasets.pt')\n",
    " \n",
    "if not disable_cuda and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make results reproducable\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 48540\n",
      "Validation samples: 12136\n"
     ]
    }
   ],
   "source": [
    "dataloader_opts = {\n",
    "    \"batch_size\": batch_size, \n",
    "    \"pin_memory\": True, \n",
    "    \"drop_last\": True,\n",
    "    \"num_workers\": os.cpu_count()\n",
    "}\n",
    "\n",
    "if prep_datasets:\n",
    "    ds_paths = [\n",
    "        \"../data/datasets/kelp/nw_calvert_2012\",\n",
    "        \"../data/datasets/kelp/nw_calvert_2015\", \n",
    "        \"../data/datasets/kelp/choked_pass_2016\",\n",
    "        \"../data/datasets/kelp/west_beach_2016\"\n",
    "    ]\n",
    "    ds = torch.utils.data.ConcatDataset([SegmentationDataset(path) for path in ds_paths])\n",
    "\n",
    "    train_num = int(len(ds) * 0.8)\n",
    "    val_num = len(ds) - train_num\n",
    "    ds_train, ds_val = torch.utils.data.random_split(ds, [train_num, val_num])\n",
    "\n",
    "    # Bind data transforms to the subset datasets\n",
    "    ds_train = TransformDataset(ds_train, T.train_transforms, T.train_target_transforms)\n",
    "    ds_val = TransformDataset(ds_val, T.test_transforms, T.test_target_transforms)\n",
    "\n",
    "    dataloaders = {\n",
    "        'train': DataLoader(ds_train, shuffle=True, **dataloader_opts),\n",
    "        'eval': DataLoader(ds_val, shuffle=False, **dataloader_opts),\n",
    "    }\n",
    "\n",
    "    print(\"Training samples:\", train_num)\n",
    "    print(\"Validation samples:\", val_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ds_pixel_stats(dataloader):\n",
    "    pixel_counts = torch.zeros((num_classes)).to(device)\n",
    "\n",
    "    for x, y in tqdm(iter(dataloader)):\n",
    "        try:\n",
    "            un, counts = torch.unique(y.to(device), return_counts=True)\n",
    "            mask = un != ignore_index\n",
    "            pixel_counts.index_add_(0, un[mask], counts[mask].float())\n",
    "        except Exception:\n",
    "            import pdb; pdb.set_trace()\n",
    "    \n",
    "    pixel_ratio = pixel_counts / pixel_counts.sum(dim=0)\n",
    "    pixel_counts = pixel_counts.detach().cpu().numpy()\n",
    "    pixel_ratio = np.around(pixel_ratio.detach().cpu().numpy(), 4)\n",
    "    \n",
    "    return pixel_ratio, pixel_counts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(ds_train)):\n",
    "#     if ds_train[i][0].shape != torch.Size([3,200,200]):\n",
    "#         print(ds_train[i][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23d339a034674867a6519b9716268c4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=12135.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[0.9657 0.0343]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "329d27c6c3d24423a4e63d325603457b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3034.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[0.9596 0.0404]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if prep_datasets:\n",
    "    train_proportions, train_counts = ds_pixel_stats(dataloaders['train'])\n",
    "    print(train_proportions)\n",
    "    plt.figure(); plt.bar(range(num_classes), train_proportions); plt.show()\n",
    "\n",
    "    val_proportions, val_counts = ds_pixel_stats(dataloaders['eval'])\n",
    "    print(val_proportions) \n",
    "    plt.figure(); plt.bar(range(num_classes), val_proportions); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Undersample kelp-free empty images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a2bcf875c564c00b334d18e6a89205d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=48540.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_indices_of_kelp_images(dataset):\n",
    "    dl = DataLoader(dataset, batch_size=1, shuffle=False, pin_memory=True, num_workers=os.cpu_count())\n",
    "    indices = []\n",
    "    \n",
    "    for i, (_, y) in enumerate(tqdm(iter(dl))):\n",
    "        if torch.any(y > 0):\n",
    "            indices.append(i)\n",
    "    return indices\n",
    "\n",
    "if prep_datasets:\n",
    "    train_indices = get_indices_of_kelp_images(ds_train)\n",
    "    val_indices = get_indices_of_kelp_images(ds_val)\n",
    "\n",
    "    ds_train = torch.utils.data.Subset(ds_train, train_indices)\n",
    "    ds_val = torch.utils.data.Subset(ds_val, val_indices)\n",
    "    ds_overfit = torch.utils.data.Subset(ds_train, range(0, batch_size))\n",
    "\n",
    "    torch.save({\n",
    "        'train': ds_train,\n",
    "        'val': ds_val,\n",
    "        'overfit': ds_overfit\n",
    "    }, dataset_path)\n",
    "else:\n",
    "    dataset_saved = torch.load(dataset_path)\n",
    "    ds_train = dataset_saved['train']\n",
    "    ds_val = dataset_saved['val']\n",
    "    ds_overfit = dataset_saved['overfit']\n",
    "    \n",
    "\n",
    "dataloaders = {\n",
    "    'train': DataLoader(ds_train, shuffle=True, **dataloader_opts),\n",
    "    'eval': DataLoader(ds_val, shuffle=False, **dataloader_opts),\n",
    "    'overfit': DataLoader(ds_overfit, shuffle=False, **dataloader_opts)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_proportions, train_counts = ds_pixel_stats(dataloaders['train'])\n",
    "print(train_proportions)\n",
    "plt.figure(); plt.bar(range(num_classes), train_proportions); plt.show()\n",
    "\n",
    "val_proportions, val_counts = ds_pixel_stats(dataloaders['eval'])\n",
    "print(val_proportions) \n",
    "plt.figure(); plt.bar(range(num_classes), val_proportions); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Net, opt, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = deeplabv3.create_model(num_classes)\n",
    "model = model.to(device)\n",
    "# model = half_precision(model)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.01)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=ignore_index)\n",
    "# from utils.loss import CBLoss\n",
    "# criterion = CBLoss(train_counts.astype(np.int32), num_classes, \"focal\", 0.9999, 2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfit to a single batch to ensure things work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def overfit_model_to_single_batch(model, dataloader, optimizer, criterion, num_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    info = OrderedDict()\n",
    "    sum_iou = np.zeros(num_classes)\n",
    "    cm = ConfusionMatrix(num_classes).to(device)\n",
    "    \n",
    "    with trange(1, num_epochs+1) as pbar:\n",
    "        for epoch in pbar:\n",
    "            for x, y in dataloader:\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                pred = model(x)['out']\n",
    "                loss = criterion(pred, y)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                if epoch % 100 == 0:\n",
    "                    show_torch_batch(x, y, pred)\n",
    "\n",
    "                # Compute metrics\n",
    "                loss = loss.detach().cpu().item()\n",
    "                info['batch_loss'] = loss\n",
    "\n",
    "                cm.update(y, pred.max(dim=1)[1])\n",
    "                info['IoUs'] = np.around(np.nan_to_num(cm.get_iou().detach().cpu().numpy()), 4)\n",
    "\n",
    "                pbar.set_postfix(info)\n",
    "    \n",
    "    return model\n",
    "    \n",
    "# model = overfit_model_to_single_batch(model, dataloaders['overfit'], optimizer, criterion, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, num_classes, optimizer, criterion, num_epochs, save_path, start_epoch=0):\n",
    "    writer = SummaryWriter()\n",
    "    info = OrderedDict()\n",
    "    \n",
    "    best_loss = None\n",
    "    \n",
    "    for epoch in trange(start_epoch, num_epochs, desc=\"epoch\"):\n",
    "        sum_loss = 0.\n",
    "        sum_iou = np.zeros(num_classes)\n",
    "        cm = ConfusionMatrix(num_classes).to(device)\n",
    "        \n",
    "        for phase in ['train', 'eval']:\n",
    "            with tqdm(iter(dataloaders[phase]), desc=phase) as pbar:\n",
    "                for i, (x, y) in enumerate(pbar):\n",
    "                    global_step = (epoch+1)*(i+1)\n",
    "                    \n",
    "                    x = x.to(device)\n",
    "                    y = y.to(device)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        model.train()\n",
    "                    else:\n",
    "                        model.eval()\n",
    "\n",
    "                    pred = model(x)['out']\n",
    "                    loss = criterion(pred, y)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                    # Compute metrics\n",
    "                    sum_loss += loss.detach().cpu().item()\n",
    "                    info['mean_loss'] = sum_loss / (i+1)\n",
    "\n",
    "                    mask = y != ignore_index\n",
    "                    cm.update(y[mask], pred.max(dim=1)[1][mask])\n",
    "                    info['IoUs'] = np.around(np.nan_to_num(cm.get_iou().detach().cpu().numpy()), 4)\n",
    "\n",
    "                    pbar.set_postfix(info)\n",
    "                    \n",
    "#                     if global_step == 1:\n",
    "#                         writer.add_graph(model, x)\n",
    "                    \n",
    "                    writer.add_scalar(f'Loss/{phase}', info['mean_loss'], global_step)\n",
    "                    writer.add_scalar(f'Mean IoU/{phase}', np.mean(info['IoUs']), global_step)\n",
    "                    writer.add_histogram(f'IoUs/{phase}', info['IoUs'], global_step, bins=num_classes)\n",
    "#                     import pdb; pdb.set_trace()\n",
    "                    if global_step % 500 == 0:\n",
    "                        # Show images\n",
    "                        img_grid = torchvision.utils.make_grid(x, nrow=x.shape[0])\n",
    "                        img_grid = T.inv_normalize(img_grid)\n",
    "\n",
    "                        # Show labels and predictions\n",
    "                        y = y.unsqueeze(dim=1)\n",
    "                        label_grid = torchvision.utils.make_grid(y, nrow=y.shape[0]).type(torch.FloatTensor).cuda()\n",
    "                        label_grid = alpha_blend(img_grid, label_grid)\n",
    "                        writer.add_image(f'{phase}/labels', label_grid, global_step)\n",
    "\n",
    "                        # Show predictions\n",
    "                        pred = pred.max(dim=1)[1].unsqueeze(dim=1)\n",
    "                        pred_grid = torchvision.utils.make_grid(pred, nrow=pred.shape[0]).type(torch.FloatTensor).cuda()\n",
    "                        pred_grid = alpha_blend(img_grid, pred_grid)\n",
    "                        writer.add_image(f'{phase}/preds', pred_grid, global_step)\n",
    "                        \n",
    "            # Show a batch of images\n",
    "            # show_torch_batch(x, y, pred)\n",
    "\n",
    "                \n",
    "        # Model checkpointing after eval stage\n",
    "        if best_loss is None or info['mean_loss'] < best_loss:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'mean_eval_loss': info['mean_loss'],\n",
    "                }, save_path)\n",
    "        \n",
    "    writer.close()\n",
    " \n",
    "    return model\n",
    "\n",
    "\n",
    "def alpha_blend(bg, fg, alpha=0.5):   \n",
    "    return fg*alpha + bg*(1-alpha)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "save_path = Path('../checkpoints/deeplabv3/checkpoint.pt')\n",
    "save_path.parents[0].mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if Path(save_path).exists():\n",
    "    checkpoint = torch.load(save_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "else:\n",
    "    epoch = 0\n",
    "\n",
    "model =  train_model(model, dataloaders, num_classes, optimizer, criterion, num_epochs, save_path, start_epoch=epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "- Change mixed class to don't care\n",
    "- Try different optimizer\n",
    "- ~~Train for Kelp/Not Kelp only~~\n",
    "- Check that changing prior doesn't affect performance too badly\n",
    "- Optimize regularization\n",
    "\n",
    "## TO TRY\n",
    "- Add Jaccard index loss as in https://www.kaggle.com/windsurfer/baseline-u-net-on-pytorch\n",
    "- Straight Jaccard loss\n",
    "- ~~Undersample Kelp-free images~~\n",
    "- Oversample images with kelp\n",
    "- Try UNet"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
