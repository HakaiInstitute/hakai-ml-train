{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "import os\n",
    "from tqdm.auto import tqdm, trange\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "from utils.dataset import SegmentationDataset, TransformDataset, transforms as T\n",
    "from models import deeplabv3, half_precision\n",
    "from utils.loss import iou\n",
    "\n",
    "use_half_precision = False\n",
    "disable_cuda = False\n",
    "num_classes = 2\n",
    "num_epochs = 200\n",
    "batch_size = 16\n",
    "ignore_index = 100  # Value of labels that we ignore in loss and other logic (e.g. kelp with unknown species)\n",
    "\n",
    "prep_datasets = False\n",
    "torch_dataset_pickle = Path('checkpoints/datasets.pt')\n",
    "torch_dataset_pickle.parents[0].mkdir(parents=True, exist_ok=True)\n",
    "ds_paths = [\n",
    "    \"data/datasets/kelp/nw_calvert_2012\",\n",
    "    \"data/datasets/kelp/nw_calvert_2015\",\n",
    "    \"data/datasets/kelp/choked_pass_2016\",\n",
    "    \"data/datasets/kelp/west_beach_2016\"\n",
    "]\n",
    "dataloader_opts = {\n",
    "    \"batch_size\": batch_size,\n",
    "    \"pin_memory\": True,\n",
    "    \"drop_last\": True,\n",
    "    \"num_workers\": os.cpu_count()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_blend(bg, fg, alpha=0.5):\n",
    "    return fg*alpha + bg*(1-alpha)\n",
    "\n",
    "\n",
    "def ds_pixel_stats(dataloader):\n",
    "    pixel_counts = torch.zeros((num_classes)).to(device)\n",
    "\n",
    "    for _, y in tqdm(iter(dataloader)):\n",
    "        try:\n",
    "            un, counts = torch.unique(y.to(device), return_counts=True)\n",
    "            mask = un != ignore_index\n",
    "            pixel_counts.index_add_(0, un[mask], counts[mask].float())\n",
    "        except Exception:\n",
    "            import pdb;\n",
    "            pdb.set_trace()\n",
    "\n",
    "    pixel_ratio = pixel_counts / pixel_counts.sum(dim=0)\n",
    "    pixel_counts = pixel_counts.detach().cpu().numpy()\n",
    "    pixel_ratio = np.around(pixel_ratio.detach().cpu().numpy(), 4)\n",
    "\n",
    "    return pixel_ratio, pixel_counts\n",
    "\n",
    "\n",
    "def get_indices_of_kelp_images(dataset):\n",
    "    dl = DataLoader(dataset, batch_size=1, shuffle=False, pin_memory=True, num_workers=os.cpu_count())\n",
    "    indices = []\n",
    "    for i, (_, y) in enumerate(tqdm(iter(dl))):\n",
    "        if torch.any(y > 0):\n",
    "            indices.append(i)\n",
    "    return indices\n",
    "\n",
    "\n",
    "def train_model(model, dataloaders, num_classes, optimizer, criterion, num_epochs, save_path, start_epoch=0):\n",
    "    writers = {\n",
    "        'train': SummaryWriter(comment='_train'),\n",
    "        'eval': SummaryWriter(comment='_eval')\n",
    "    }\n",
    "    info = OrderedDict()\n",
    "\n",
    "    best_loss = None\n",
    "    pbar_epoch = trange(num_epochs, desc=\"epoch\")\n",
    "    if start_epoch != 0:\n",
    "        pbar_epoch.update(start_epoch)\n",
    "        pbar_epoch.refresh()\n",
    "    \n",
    "    for epoch in pbar_epoch:\n",
    "        for phase in ['train', 'eval']:\n",
    "            sum_loss = 0.\n",
    "            sum_iou = np.zeros(num_classes)\n",
    "\n",
    "            with tqdm(iter(dataloaders[phase]), desc=phase) as pbar:\n",
    "                for i, (x, y) in enumerate(pbar):\n",
    "                    y = y.to(device)\n",
    "                    x = x.to(device)\n",
    "                    if use_half_precision:\n",
    "                        x = x.half()\n",
    "                        \n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        model.train()\n",
    "                    else:\n",
    "                        model.eval()\n",
    "\n",
    "                    pred = model(x)['out']\n",
    "                    loss = criterion(pred.float(), y)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                    # Compute metrics\n",
    "                    sum_loss += loss.detach().cpu().item()\n",
    "                    info['mean_loss'] = sum_loss / (i + 1)\n",
    "                    sum_iou += iou(y, pred.float()).detach().cpu().numpy()\n",
    "                    info['mIoUs'] = np.around(sum_iou / (i + 1), 4)\n",
    "\n",
    "                    pbar.set_postfix(info)\n",
    "\n",
    "                writers[phase].add_scalar('Loss', info['mean_loss'], epoch)\n",
    "                writers[phase].add_scalar('IoU/Mean', np.mean(info['mIoUs']), epoch)\n",
    "                writers[phase].add_scalar('IoU/BG', sum_iou[0] / (i+1), epoch)\n",
    "                writers[phase].add_scalar('IoU/Kelp', sum_iou[1] / (i+1), epoch)\n",
    "\n",
    "                img_grid = torchvision.utils.make_grid(x, nrow=8)\n",
    "                img_grid = T.inv_normalize(img_grid)\n",
    "\n",
    "                # Show labels and predictions\n",
    "                y = y.unsqueeze(dim=1)\n",
    "                label_grid = torchvision.utils.make_grid(y, nrow=8).cuda()\n",
    "                label_grid = alpha_blend(img_grid, label_grid)\n",
    "                writers[phase].add_image('Labels/True', label_grid, epoch)\n",
    "\n",
    "                # Show predictions\n",
    "                pred = pred.max(dim=1)[1].unsqueeze(dim=1)\n",
    "                pred_grid = torchvision.utils.make_grid(pred, nrow=8).cuda()\n",
    "                pred_grid = alpha_blend(img_grid, pred_grid)\n",
    "                writers[phase].add_image('Labels/Pred', pred_grid, epoch)\n",
    "\n",
    "        # Model checkpointing after eval stage\n",
    "        if best_loss is None or info['mean_loss'] < best_loss:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'mean_eval_loss': info['mean_loss'],\n",
    "            }, save_path)\n",
    "\n",
    "    pbar_epoch.close()\n",
    "    writers['train'].flush()\n",
    "    writers['train'].close()\n",
    "    writers['eval'].flush()\n",
    "    writers['eval'].close()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5061b621f681454b858a3e9d2852bcbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='epoch', max=200.0, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d28aadd9784d4e1cb129177f3bf9cad9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='train', max=388.0, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    if not disable_cuda and torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "\n",
    "    print(\"Using device:\", device)\n",
    "\n",
    "    # Make results reproducable\n",
    "    torch.manual_seed(0)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(0)\n",
    "\n",
    "    # Datasets\n",
    "    if prep_datasets:\n",
    "        ds = torch.utils.data.ConcatDataset([SegmentationDataset(path) for path in ds_paths])\n",
    "\n",
    "        train_num = int(len(ds) * 0.8)\n",
    "        val_num = len(ds) - train_num\n",
    "        ds_train, ds_val = torch.utils.data.random_split(ds, [train_num, val_num])\n",
    "\n",
    "        # Bind data transforms to the subset datasets\n",
    "        ds_train = TransformDataset(ds_train, T.train_transforms, T.train_target_transforms)\n",
    "        ds_val = TransformDataset(ds_val, T.test_transforms, T.test_target_transforms)\n",
    "\n",
    "        train_indices = get_indices_of_kelp_images(ds_train)\n",
    "        val_indices = get_indices_of_kelp_images(ds_val)\n",
    "\n",
    "        ds_train = torch.utils.data.Subset(ds_train, train_indices)\n",
    "        ds_val = torch.utils.data.Subset(ds_val, val_indices)\n",
    "\n",
    "        torch.save({\n",
    "            'train': ds_train,\n",
    "            'val': ds_val,\n",
    "        }, torch_dataset_pickle)\n",
    "\n",
    "    else:\n",
    "        dataset_saved = torch.load(torch_dataset_pickle)\n",
    "        ds_train = dataset_saved['train']\n",
    "        ds_val = dataset_saved['val']\n",
    "\n",
    "    # Net, opt, loss, dataloaders\n",
    "    model = deeplabv3.create_model(num_classes)\n",
    "    model = model.to(device)\n",
    "    if use_half_precision:\n",
    "        model = half_precision(model)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=ignore_index)\n",
    "\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    if num_gpus > 1:\n",
    "        print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n",
    "        batch_size *= num_gpus\n",
    "        model = nn.DataParallel(model)\n",
    "\n",
    "    data_loaders = {\n",
    "        'train': DataLoader(ds_train, shuffle=True, **dataloader_opts),\n",
    "        'eval': DataLoader(ds_val, shuffle=False, **dataloader_opts),\n",
    "    }\n",
    "\n",
    "    # Train the model\n",
    "    save_path = Path('checkpoints/deeplabv3/checkpoint.pt')\n",
    "    save_path.parents[0].mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Restart at checkpoint if it exists\n",
    "    if Path(save_path).exists():\n",
    "        checkpoint = torch.load(save_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        epoch = checkpoint['epoch']\n",
    "    else:\n",
    "        epoch = 0\n",
    "\n",
    "    model = train_model(model, data_loaders, num_classes, optimizer, criterion, num_epochs, save_path,\n",
    "                        start_epoch=epoch)\n",
    "\n",
    "    # Save the final model\n",
    "    save_path = Path(f'checkpoints/deeplabv3/deeplabv3_epoch{num_epochs}.pt')\n",
    "    save_path.parents[0].mkdir(parents=True, exist_ok=True)\n",
    "    torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "- Change mixed class to don't care\n",
    "- Try different optimizer\n",
    "- ~~Train for Kelp/Not Kelp only~~\n",
    "- Check that changing prior doesn't affect performance too badly\n",
    "- Optimize regularization\n",
    "\n",
    "## TO TRY\n",
    "- Add Jaccard index loss as in https://www.kaggle.com/windsurfer/baseline-u-net-on-pytorch\n",
    "- Straight Jaccard loss\n",
    "- ~~Undersample Kelp-free images~~\n",
    "- Oversample images with kelp\n",
    "- Try UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
