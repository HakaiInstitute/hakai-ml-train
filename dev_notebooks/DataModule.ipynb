{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02fee9ff-dedc-4eab-b706-7716ea90c42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from PIL import Image as PILImage\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import tv_tensors\n",
    "from torchvision.datasets import VisionDataset\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.tv_tensors import Image, Mask\n",
    "\n",
    "\n",
    "class SegmentationDataset(VisionDataset):\n",
    "    \"\"\"Load preprocessed image chips. Used during model train and validation phases.\"\"\"\n",
    "\n",
    "    def __init__(self, root: str, *args, ext: str = \"tif\", **kwargs):\n",
    "        super().__init__(root, *args, **kwargs)\n",
    "\n",
    "        self._images = sorted(Path(root).joinpath(\"x\").glob(f\"*.{ext}\"))\n",
    "        self._labels = sorted(Path(root).joinpath(\"y\").glob(f\"*.{ext}\"))\n",
    "\n",
    "        assert len(self._images) == len(self._labels), (\n",
    "            \"There are an unequal number of images and labels!\"\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._images)\n",
    "\n",
    "    # noinspection DuplicatedCode\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image(PILImage.open(self._images[idx]))\n",
    "        target = Mask(PILImage.open(self._labels[idx]))\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            with torch.no_grad():\n",
    "                img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "\n",
    "class PadOut:\n",
    "    def __init__(self, height: int = 128, width: int = 128, fill_value: int = 0):\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.fill_value = fill_value\n",
    "\n",
    "    def __call__(self, x: Any) -> Any:\n",
    "        \"\"\"\n",
    "        Pad out a pillow image, so it is the correct size as specified by `self.height` and `self.width`\n",
    "        \"\"\"\n",
    "        h, w = x.shape[-2:]\n",
    "\n",
    "        if h == self.height and w == self.width:\n",
    "            return x\n",
    "\n",
    "        w_pad = self.width - w\n",
    "        h_pad = self.height - h\n",
    "\n",
    "        return tv_tensors.wrap(\n",
    "            v2.functional.pad(x, [0, 0, w_pad, h_pad], fill=self.fill_value), like=x\n",
    "        )\n",
    "\n",
    "\n",
    "def normalize_min_max(x: torch.Tensor) -> torch.Tensor:\n",
    "    mask = torch.all(x == 0, dim=0).unsqueeze(dim=0).repeat(x.shape[0], 1, 1)\n",
    "    masked_values = x.flatten()[~mask.flatten()]\n",
    "\n",
    "    min_ = masked_values.min()\n",
    "    max_ = masked_values.max()\n",
    "    return tv_tensors.wrap(torch.clamp((x - min_) / (max_ - min_), 0, 1), like=x)\n",
    "\n",
    "\n",
    "def normalize_min_max2(x: torch.Tensor) -> torch.Tensor:\n",
    "    # Get second smallest values to accomodate black backgrounds/nodata areas\n",
    "    min_, _ = torch.kthvalue(x.flatten().unique(), 2)\n",
    "    max_ = x.flatten().max()\n",
    "    return tv_tensors.wrap(torch.clamp((x - min_) / (max_ - min_), 0, 1), like=x)\n",
    "\n",
    "\n",
    "def normalize_percentile(x: torch.Tensor, upper=0.99, lower=0.01) -> torch.Tensor:\n",
    "    mask = torch.all(x == 0, dim=0).unsqueeze(dim=0).repeat(x.shape[0], 1, 1)\n",
    "    masked_values = x.flatten()[~mask.flatten()]\n",
    "\n",
    "    max_ = torch.quantile(masked_values, upper)\n",
    "    min_ = torch.quantile(masked_values, lower)\n",
    "    return tv_tensors.wrap(torch.clamp((x - min_) / (max_ - min_), 0, 1), like=x)\n",
    "\n",
    "\n",
    "def append_ndvi(x: torch.Tensor) -> torch.Tensor:\n",
    "    r, g, b, nir = x\n",
    "    ndvi = torch.nan_to_num(torch.div((nir - r), (nir + r)))\n",
    "\n",
    "    # Scale NDVI from [-1, 1] to [0, 255]\n",
    "    ndvi = (((ndvi + 1) / 2.0) * 255).to(torch.uint8)\n",
    "\n",
    "    new_x = torch.concat((x, ndvi.unsqueeze(dim=0)), dim=0)\n",
    "    return tv_tensors.wrap(new_x, like=x)\n",
    "\n",
    "\n",
    "# normalize = t.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "\n",
    "# noinspection PyAbstractClass\n",
    "class DataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir: str,\n",
    "        num_classes: int,\n",
    "        batch_size: int,\n",
    "        num_workers: int = os.cpu_count(),\n",
    "        pin_memory: bool = True,\n",
    "        persistent_workers: bool = False,\n",
    "        fill_value: int = 0,\n",
    "        tile_size: int = 1024,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.pin_memory = pin_memory\n",
    "        self.persistent_workers = persistent_workers\n",
    "        self.fill_value = fill_value\n",
    "        self.tile_size = tile_size\n",
    "\n",
    "        self.train_data_dir = Path(data_dir).joinpath(\"train\")\n",
    "        self.val_data_dir = Path(data_dir).joinpath(\"val\")\n",
    "        self.test_data_dir = Path(data_dir).joinpath(\"test\")\n",
    "\n",
    "        self.pad_f = PadOut(self.tile_size, self.tile_size, fill_value=self.fill_value)\n",
    "\n",
    "        self.train_trans = v2.Compose(\n",
    "            [\n",
    "                v2.RandomHorizontalFlip(),\n",
    "                v2.RandomVerticalFlip(),\n",
    "                # t.RandomRotation(degrees=15, fill=self.fill_value),\n",
    "                # t.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "                # v2.Lambda(self.pad_f, Image, Mask),\n",
    "                # v2.Lambda(append_ndvi, Image),\n",
    "                v2.Lambda(normalize_min_max2, Image),\n",
    "                v2.Lambda(v2.ToDtype(torch.float16, scale=True), Image),\n",
    "                v2.Lambda(v2.ToDtype(torch.long), Mask),\n",
    "                v2.Lambda(torch.squeeze, Mask),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.test_trans = v2.Compose(\n",
    "            [\n",
    "                # v2.Lambda(self.pad_f, Image, Mask),\n",
    "                # v2.Lambda(append_ndvi, Image),\n",
    "                v2.Lambda(normalize_min_max2, Image),\n",
    "                v2.Lambda(v2.ToDtype(torch.float16, scale=True), Image),\n",
    "                v2.Lambda(v2.ToDtype(torch.long), Mask),\n",
    "                v2.Lambda(torch.squeeze, Mask),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.ds_train, self.ds_val, self.ds_test = None, None, None\n",
    "\n",
    "    def prepare_data(self, *args, **kwargs):\n",
    "        pass\n",
    "\n",
    "    def setup(self, stage: str | None = None):\n",
    "        self.ds_train = SegmentationDataset(\n",
    "            self.train_data_dir,\n",
    "            ext=\"tif\",\n",
    "            transforms=self.train_trans,\n",
    "        )\n",
    "        self.ds_val = SegmentationDataset(\n",
    "            self.val_data_dir,\n",
    "            ext=\"tif\",\n",
    "            transforms=self.test_trans,\n",
    "        )\n",
    "        self.ds_test = SegmentationDataset(\n",
    "            self.test_data_dir,\n",
    "            ext=\"tif\",\n",
    "            transforms=self.test_trans,\n",
    "        )\n",
    "\n",
    "    def teardown(self, stage: str | None = None) -> None:\n",
    "        del self.ds_train\n",
    "        del self.ds_val\n",
    "        del self.ds_test\n",
    "\n",
    "    def train_dataloader(self, *args, **kwargs) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.ds_train,\n",
    "            shuffle=True,\n",
    "            batch_size=self.batch_size,\n",
    "            pin_memory=self.pin_memory,\n",
    "            drop_last=True,\n",
    "            num_workers=self.num_workers,\n",
    "            persistent_workers=self.persistent_workers,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self, *args, **kwargs) -> DataLoader | list[DataLoader]:\n",
    "        return DataLoader(\n",
    "            self.ds_val,\n",
    "            shuffle=False,\n",
    "            batch_size=self.batch_size,\n",
    "            pin_memory=self.pin_memory,\n",
    "            num_workers=self.num_workers,\n",
    "            persistent_workers=self.persistent_workers,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self, *args, **kwargs) -> DataLoader | list[DataLoader]:\n",
    "        return DataLoader(\n",
    "            self.ds_test,\n",
    "            shuffle=False,\n",
    "            batch_size=self.batch_size,\n",
    "            pin_memory=self.pin_memory,\n",
    "            num_workers=self.num_workers,\n",
    "            persistent_workers=self.persistent_workers,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "432b9986-f1c5-436a-add8-a13c8d3d4399",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = DataModule(\n",
    "    data_dir=\"/home/taylor/data/KP-ACO-RGBI-Nov2023\",\n",
    "    num_classes=3,\n",
    "    batch_size=4,\n",
    "    num_workers=1,\n",
    "    pin_memory=False,\n",
    "    persistent_workers=False,\n",
    "    fill_value=0,\n",
    "    tile_size=1024,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f5553ff1-65b5-4de2-b57c-e97ef1f04025",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "d.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "277e8205-9490-4dd6-b785-578698947c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "td = d.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6bf7379a-b89d-4907-8077-304ae0af6ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in td:\n",
    "    x = x\n",
    "    y = y\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2240011a-9864-4f49-801f-570920e24a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "d.ds_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "86d3d7b2-ff65-41dd-a882-44c1cc77268e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.shape, y.shape)\n",
    "print(x.dtype, y.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e1ecc4bf-21fd-4213-8682-6efcd24694ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "\n",
    "\n",
    "def vis_image(img, label):\n",
    "    masks = torch.nn.functional.one_hot(label.to(torch.int64), num_classes=3)[\n",
    "        :, :, :, 1:\n",
    "    ].to(torch.bool)\n",
    "\n",
    "    for x, y in zip(img, masks, strict=False):\n",
    "        x = v2.functional.to_dtype(x, torch.uint8, scale=True)[:3]\n",
    "        y = y.moveaxis(2, 0)\n",
    "\n",
    "        im = torchvision.utils.draw_segmentation_masks(\n",
    "            x, y, colors=[\"green\", \"yellow\", \"blue\", \"red\"], alpha=0.4\n",
    "        )\n",
    "\n",
    "        plt.figure()\n",
    "        plt.imshow(im.moveaxis(0, 2))\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "vis_image(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11846980-3006-4abd-a338-efe65b7436e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, axis = plt.subplots(1, 3, figsize=(10, 10))\n",
    "\n",
    "im = x[1]\n",
    "axis[0].imshow(im[:3].moveaxis(0, 2))\n",
    "axis[0].set_title(\"Raw\")\n",
    "\n",
    "im2 = normalize_min_max(im.clone())\n",
    "axis[1].imshow(im2[:3].moveaxis(0, 2))\n",
    "axis[1].set_title(\"minmax\")\n",
    "\n",
    "im3 = normalize_percentile(im.clone(), upper=0.99, lower=0.01)\n",
    "axis[2].imshow(im3[:3].moveaxis(0, 2))\n",
    "axis[2].set_title(\"1/99 perc.\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55668463-5626-4ab2-8c2a-007599fe9836",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ec1c96-4c3f-419a-87ad-8ce09772e9ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
