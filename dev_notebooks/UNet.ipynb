{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ceda5439-a94e-4661-962d-d6518a392f77",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "from pathlib import Path\n",
    "\n",
    "print(Path.cwd())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bff596f-4875-4a35-9c95-201f3c442f0d",
   "metadata": {},
   "source": [
    "%set_env WANDB_NOTEBOOK_NAME \"/mnt/data/Taylor/notebooks/KOM-kelp-aco/UNet.ipynb\""
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fbe66ac-0f3e-46ef-98bc-87afb01c4d7b",
   "metadata": {},
   "source": [
    "import os\n",
    "import random\n",
    "from argparse import ArgumentParser\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Union, Any\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torchmetrics.functional as fm\n",
    "import wandb\n",
    "from PIL import Image\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchmetrics.classification import Dice\n",
    "from torchvision import transforms as t\n",
    "from torchvision.transforms.functional import pad, pil_to_tensor\n",
    "from wandb import AlertLevel\n",
    "import torch.nn.functional as F\n",
    "import segmentation_models_pytorch as smp\n",
    "from lightning.pytorch.profilers import SimpleProfiler\n",
    "\n",
    "from einops import rearrange\n",
    "from unified_focal_loss import AsymmetricUnifiedFocalLoss, FocalTverskyLoss\n",
    "from datamodule import DataModule"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "4f787f52-20c5-4dc6-b2dd-647e125d8245",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d75d130-973a-47bd-8159-4b23589495f0",
   "metadata": {},
   "source": [
    "# CHECKPOINT OPTIONS\n",
    "CHECKPOINT_DIR = \"./checkpoints\"\n",
    "NAME = \"UNet\"\n",
    "PROJECT_NAME = \"kom-kelp-pa-aco-rgbi\"\n",
    "\n",
    "# DATASET OPTIONS\n",
    "DATA_DIR = \"/home/taylor/data/KP-ACO-RGBI-Nov2023/\"\n",
    "NUM_WORKERS = os.cpu_count() // 2\n",
    "PIN_MEMORY = True\n",
    "NUM_CLASSES = 3\n",
    "IGNORE_INDEX = 2\n",
    "BATCH_SIZE = 2\n",
    "FILL_VALUE = 0\n",
    "NUM_BANDS = 4\n",
    "\n",
    "# MODEL OPTIONS\n",
    "LR = 0.0003\n",
    "ALPHA = 0.8\n",
    "GAMMA = 0.5\n",
    "WEIGHT_DECAY = 0.0001\n",
    "MAX_EPOCHS = 10\n",
    "PRECISION = \"16-mixed\"\n",
    "SYNC_BATCHNORM = True\n",
    "IMG_SHAPE = 1024\n",
    "DROPOUT = 0.5\n",
    "WARMUP_PERIOD = 1.0 / MAX_EPOCHS"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fc852ec-ed9d-469e-9ffe-afc0a0da9e45",
   "metadata": {},
   "source": [
    "pl.seed_everything(0, workers=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edcce279-d91c-49a5-93bb-6e01c739f594",
   "metadata": {},
   "source": [
    "# Make checkpoint directory\n",
    "Path(CHECKPOINT_DIR, NAME).mkdir(exist_ok=True, parents=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "11751207-5ded-4c09-98b3-e8d9147d5092",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d770b3a-75f7-4089-a2ca-0b2b5ba3b508",
   "metadata": {},
   "source": [
    "# Created by: Taylor Denouden\n",
    "# Organization: Hakai Institute\n",
    "\n",
    "\n",
    "class UNet(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: int = 2,\n",
    "        ignore_index: Optional[int] = None,\n",
    "        lr: float = 0.35,\n",
    "        weight_decay: float = 0,\n",
    "        loss_delta: float = 0.7,\n",
    "        loss_gamma: float = 4.0 / 3.0,\n",
    "        max_epochs: int = 100,\n",
    "        warmup_period: float = 0.3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.ignore_index = ignore_index\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.max_epochs = max_epochs\n",
    "        self.warmup_period = warmup_period\n",
    "\n",
    "        self.loss_delta = loss_delta\n",
    "        if self.ignore_index is not None:\n",
    "            self.n = num_classes - 1\n",
    "        else:\n",
    "            self.n = num_classes\n",
    "\n",
    "        self.model = smp.UnetPlusPlus(\n",
    "            \"resnet50\",\n",
    "            in_channels=NUM_BANDS,\n",
    "            classes=self.n,\n",
    "            decoder_attention_type=\"scse\",\n",
    "        )\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = True\n",
    "        self.model = torch.compile(self.model, fullgraph=False, mode=\"max-autotune\")\n",
    "        self.loss_fn = AsymmetricUnifiedFocalLoss(delta=loss_delta, gamma=loss_gamma)\n",
    "        # self.loss_fn = FocalTverskyLoss(delta=loss_delta, gamma=loss_gamma)\n",
    "\n",
    "    @property\n",
    "    def example_input_array(self) -> Any:\n",
    "        return torch.ones(\n",
    "            (BATCH_SIZE, NUM_BANDS, IMG_SHAPE, IMG_SHAPE),\n",
    "            device=self.device,\n",
    "            dtype=self.dtype,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model.forward(x)\n",
    "\n",
    "    def remove_ignore_pixels(self, logits, y):\n",
    "        mask = y != self.ignore_index\n",
    "        return logits[mask], y[mask]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self._phase_step(batch, batch_idx, phase=\"train\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self._phase_step(batch, batch_idx, phase=\"val\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self._phase_step(batch, batch_idx, phase=\"test\")\n",
    "\n",
    "    def _phase_step(self, batch, batch_idx, phase):\n",
    "        x, y = batch\n",
    "\n",
    "        logits = self.forward(x)\n",
    "\n",
    "        has_wandb_logger = isinstance(trainer.logger, pl.loggers.WandbLogger)\n",
    "        if phase == \"val\" and batch_idx == 0 and has_wandb_logger:\n",
    "            class_labels = {0: \"background\", 1: \"kelp\"}\n",
    "            images = [im for im in x[:, :3, :, :]]\n",
    "            masks = [\n",
    "                {\n",
    "                    \"predictions\": {\n",
    "                        \"mask_data\": pr.detach().cpu().numpy(),\n",
    "                        \"class_labels\": class_labels,\n",
    "                    },\n",
    "                    \"ground_truth\": {\n",
    "                        \"mask_data\": gt.detach().cpu().numpy(),\n",
    "                        \"class_labels\": class_labels,\n",
    "                    },\n",
    "                }\n",
    "                for gt, pr in zip(y, logits.argmax(dim=1))\n",
    "            ]\n",
    "            self.logger.log_image(\"Predictions\", images=images, masks=masks)\n",
    "\n",
    "        # Flatten and eliminate ignore class instances\n",
    "        y = rearrange(y, \"b h w -> (b h w)\").long()\n",
    "        logits = rearrange(logits, \"b c h w -> (b h w) c\")\n",
    "        if self.ignore_index is not None:\n",
    "            logits, y = self.remove_ignore_pixels(logits, y)\n",
    "\n",
    "        if len(y) == 0:\n",
    "            print(\"0 length y!\")\n",
    "            return 0\n",
    "\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "\n",
    "        loss = self.loss_fn(probs, y.long())\n",
    "\n",
    "        accuracy = fm.accuracy(\n",
    "            probs, y, task=\"multiclass\", num_classes=self.n, average=\"macro\"\n",
    "        )\n",
    "        miou = fm.jaccard_index(\n",
    "            probs, y, task=\"multiclass\", num_classes=self.n, average=\"macro\"\n",
    "        )\n",
    "        ious = fm.jaccard_index(\n",
    "            probs, y, task=\"multiclass\", num_classes=self.n, average=\"none\"\n",
    "        )\n",
    "        recalls = fm.recall(\n",
    "            probs, y, task=\"multiclass\", num_classes=self.n, average=\"none\"\n",
    "        )\n",
    "        precisions = fm.precision(\n",
    "            probs, y, task=\"multiclass\", num_classes=self.n, average=\"none\"\n",
    "        )\n",
    "        f1s = fm.f1_score(\n",
    "            probs, y, task=\"multiclass\", num_classes=self.n, average=\"none\"\n",
    "        )\n",
    "\n",
    "        is_training = phase == \"train\"\n",
    "        self.log(f\"{phase}/loss\", loss, prog_bar=is_training)\n",
    "        self.log(f\"{phase}/miou\", miou, prog_bar=is_training)\n",
    "        self.log(f\"{phase}/accuracy\", accuracy)\n",
    "\n",
    "        for c in range(self.n):\n",
    "            clsx = (\n",
    "                f\"cls{c + 1}\"\n",
    "                if self.ignore_index and c >= self.ignore_index\n",
    "                else f\"cls{c}\"\n",
    "            )\n",
    "            self.log(f\"{phase}/{clsx}_iou\", ious[c], prog_bar=is_training)\n",
    "            self.log_dict(\n",
    "                {\n",
    "                    f\"{phase}/{clsx}_recall\": recalls[c],\n",
    "                    f\"{phase}/{clsx}_precision\": precisions[c],\n",
    "                    f\"{phase}/{clsx}_f1\": f1s[c],\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Init optimizer and scheduler\"\"\"\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            filter(lambda p: p.requires_grad, self.parameters()),\n",
    "            lr=self.lr,\n",
    "            weight_decay=self.weight_decay,\n",
    "            amsgrad=True,\n",
    "        )\n",
    "\n",
    "        steps = self.trainer.estimated_stepping_batches\n",
    "        warmup_steps = steps * self.warmup_period\n",
    "\n",
    "        linear_warmup_sch = torch.optim.lr_scheduler.LinearLR(\n",
    "            optimizer, start_factor=0.001, total_iters=warmup_steps\n",
    "        )\n",
    "        cosine_sch = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, T_max=(steps - warmup_steps)\n",
    "        )\n",
    "        lr_scheduler = torch.optim.lr_scheduler.SequentialLR(\n",
    "            optimizer, [linear_warmup_sch, cosine_sch], milestones=[warmup_steps]\n",
    "        )\n",
    "\n",
    "        return [optimizer], [{\"scheduler\": lr_scheduler, \"interval\": \"step\"}]"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "81a79524-53d2-459e-ac5c-1bf12473a18c",
   "metadata": {},
   "source": [
    "# Setup Callbacks and Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56587af0-e811-4ea9-b3d9-721cfbf6bcfa",
   "metadata": {},
   "source": [
    "ENABLE_LOGGING = True\n",
    "\n",
    "profiler = SimpleProfiler(dirpath=\".\", filename=\"perf_logs\")\n",
    "\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    monitor=\"val/miou\",\n",
    "    mode=\"max\",\n",
    "    filename=\"{val/miou:.4f}_{epoch}\",\n",
    "    save_top_k=1,\n",
    "    save_last=True,\n",
    "    save_on_train_epoch_end=False,\n",
    "    every_n_epochs=1,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "if ENABLE_LOGGING:\n",
    "    logger = WandbLogger(\n",
    "        name=NAME,\n",
    "        project=PROJECT_NAME,\n",
    "        save_dir=CHECKPOINT_DIR,\n",
    "        log_model=True,\n",
    "    )\n",
    "    logger.experiment.config[\"batch_size\"] = BATCH_SIZE\n",
    "    # logger.experiment.config[\"dropout\"] = DROPOUT\n",
    "else:\n",
    "    logger = pl.loggers.CSVLogger(save_dir=\"/tmp/\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    # profiler=profiler,\n",
    "    # overfit_batches=10,\n",
    "    # log_every_n_steps=3,\n",
    "    # limit_train_batches=3,\n",
    "    # limit_val_batches=3,\n",
    "    # accelerator='cpu',\n",
    "    # fast_dev_run=True,\n",
    "    deterministic=True,\n",
    "    benchmark=True,\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    precision=PRECISION,\n",
    "    logger=logger,\n",
    "    gradient_clip_val=0.5,\n",
    "    accumulate_grad_batches=8,\n",
    "    callbacks=[\n",
    "        checkpoint_callback,\n",
    "        pl.callbacks.LearningRateMonitor(),\n",
    "        #         Finetuning(unfreeze_at_epoch=FINETUNE_EPOCH)\n",
    "    ],\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "1c974b14-1ae3-42c3-9d10-f973434e6dc3",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7789d51d-0a21-4e55-bf8b-cda0d978c2d1",
   "metadata": {},
   "source": [
    "data_module = DataModule(\n",
    "    DATA_DIR,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY,\n",
    "    persistent_workers=True,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    fill_value=FILL_VALUE,\n",
    "    tile_size=IMG_SHAPE,\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "71840b0b-2b39-41f6-974d-1bbd992c40c3",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f759cd7-348c-4ad4-a942-196a2ef9507e",
   "metadata": {},
   "source": [
    "# init the model directly on the device and with parameters in half-precision\n",
    "model = UNet(\n",
    "    num_classes=NUM_CLASSES,\n",
    "    ignore_index=IGNORE_INDEX,\n",
    "    lr=LR,\n",
    "    loss_delta=ALPHA,\n",
    "    loss_gamma=GAMMA,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    # dropout=DROPOUT,\n",
    "    warmup_period=WARMUP_PERIOD,\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "75ec15f2-193f-4d04-91ec-e3b447a5f45e",
   "metadata": {},
   "source": [
    "# TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "009ac9be-3927-4adb-ac0c-04ce13f1c5ae",
   "metadata": {},
   "source": [
    "# %%debug\n",
    "\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "\n",
    "try:\n",
    "    trainer.fit(model, datamodule=data_module)\n",
    "\n",
    "    if not trainer.fast_dev_run and ENABLE_LOGGING:\n",
    "        best_miou = checkpoint_callback.best_model_score.detach().cpu()\n",
    "        print(\"Best mIoU:\", best_miou)\n",
    "        wandb.alert(\n",
    "            title=\"Training complete\",\n",
    "            text=f\"Best mIoU: {best_miou}\",\n",
    "            level=AlertLevel.INFO,\n",
    "        )\n",
    "\n",
    "finally:\n",
    "    if ENABLE_LOGGING:\n",
    "        wandb.finish()\n",
    "    else:\n",
    "        shutil.rmtree(logger.log_dir)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca1f11ec-7fb5-40be-be32-a6389e2975db",
   "metadata": {},
   "source": [
    "!jupyter nbconvert --to script \"UNet.ipynb\""
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ce7239-0ede-44e4-9020-272b75e87d3c",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
