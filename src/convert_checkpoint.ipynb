{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "from lit_lraspp_mobilenet_v3_large import LRASPPMobileNetV3Large\n",
    "def train():\n",
    "    pass\n",
    "\n",
    "DEVICE = torch.device('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "weights_path = \"/home/taylor/PycharmProjects/hakai-ml-train/checkpoints/mussels/LRASPP/best-val_miou=0.8405-epoch=27-step=3387.ckpt\"\n",
    "torchscript_path = \"/home/taylor/PycharmProjects/hakai-ml-train/checkpoints/mussels/LRASPP/LRASPP_MobileNetV3_mussel_presence_jit.pt\"\n",
    "model = LRASPPMobileNetV3Large.load_from_checkpoint(weights_path, train=False, map_location=DEVICE, strict=False)\n",
    "# model.to_torchscript(file_path=torchscript_path, method='trace', example_inputs=x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Save state_dict only\n",
    "torch.save(model.state_dict(), Path(weights_path).with_suffix(\".pt\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "\n",
    "x = torch.rand(1, 3, 8, 8, device=DEVICE, requires_grad=False)\n",
    "\n",
    "model.eval()\n",
    "model.requires_grad_(False)\n",
    "with torch.no_grad():\n",
    "    traced_model = torch.jit.trace(model, example_inputs=x)\n",
    "    traced_model.save(torchscript_path)\n",
    "# torch.onnx.export(model, x, \"torchscript_files/LRASPP_MobileNetV3_kelp_species.onnx\",\n",
    "#                   opset_version=11, verbose=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'onnx_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-33-ca0d8ad7192b>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m torch.onnx.export(model, x, onnx_path,\n\u001B[0m\u001B[1;32m      2\u001B[0m                   \u001B[0minput_names\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'input'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m                   \u001B[0moutput_names\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'output'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m                   dynamic_axes={\n\u001B[1;32m      5\u001B[0m                       'input': {\n",
      "\u001B[0;31mNameError\u001B[0m: name 'onnx_path' is not defined"
     ]
    }
   ],
   "source": [
    "# torch.onnx.export(model, x, onnx_path,\n",
    "#                   input_names=['input'],\n",
    "#                   output_names=['output'],\n",
    "#                   dynamic_axes={\n",
    "#                       'input': {\n",
    "#                           0: 'batch_size',\n",
    "#                           2: 'width',\n",
    "#                           3: 'length'\n",
    "#                       },\n",
    "#                       'output': {\n",
    "#                           0: 'batch_size',\n",
    "#                           2: 'width',\n",
    "#                           3: 'length'\n",
    "#                       }\n",
    "#                   })"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: onnx in /home/taylor/miniconda3/envs/hakai-ml-train/lib/python3.9/site-packages (1.10.2)\r\n",
      "Requirement already satisfied: onnxruntime-gpu in /home/taylor/miniconda3/envs/hakai-ml-train/lib/python3.9/site-packages (1.10.0)\r\n",
      "Requirement already satisfied: numpy>=1.16.6 in /home/taylor/miniconda3/envs/hakai-ml-train/lib/python3.9/site-packages (from onnx) (1.21.2)\r\n",
      "Requirement already satisfied: protobuf in /home/taylor/miniconda3/envs/hakai-ml-train/lib/python3.9/site-packages (from onnx) (3.17.2)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.2.1 in /home/taylor/miniconda3/envs/hakai-ml-train/lib/python3.9/site-packages (from onnx) (4.0.1)\r\n",
      "Requirement already satisfied: six in /home/taylor/miniconda3/envs/hakai-ml-train/lib/python3.9/site-packages (from onnx) (1.16.0)\r\n",
      "Requirement already satisfied: flatbuffers in /home/taylor/miniconda3/envs/hakai-ml-train/lib/python3.9/site-packages (from onnxruntime-gpu) (2.0)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# % pip install onnx onnxruntime-gpu"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# import onnx\n",
    "#\n",
    "# onnx_model = onnx.load(onnx_path)\n",
    "# onnx.checker.check_model(onnx_model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n"
     ]
    }
   ],
   "source": [
    "# import onnxruntime as ort\n",
    "# import numpy as np\n",
    "#\n",
    "# z = np.ones((1, 3, 128, 128)).astype(np.float32)\n",
    "# print(z.dtype)\n",
    "# ort_sess = ort.InferenceSession(onnx_path,\n",
    "#                                 providers=['TensorrtExecutionProvider', 'CUDAExecutionProvider',\n",
    "#                                            'CPUExecutionProvider'])\n",
    "# outputs = ort_sess.run(None, {'input': z})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n"
     ]
    }
   ],
   "source": [
    "print(np.argmax(np.asarray(outputs[0]), axis=1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[[[0.9, 0.9, 0.9, ..., 0.9, 0.9, 0.9],\n         [0.9, 0.9, 0.9, ..., 0.9, 0.9, 0.9],\n         [0.9, 0.9, 0.9, ..., 0.9, 0.9, 0.9],\n         ...,\n         [0.9, 0.9, 0.9, ..., 0.9, 0.9, 0.9],\n         [0.9, 0.9, 0.9, ..., 0.9, 0.9, 0.9],\n         [0.9, 0.9, 0.9, ..., 0.9, 0.9, 0.9]],\n\n        [[0.8, 0.8, 0.8, ..., 0.8, 0.8, 0.8],\n         [0.8, 0.8, 0.8, ..., 0.8, 0.8, 0.8],\n         [0.8, 0.8, 0.8, ..., 0.8, 0.8, 0.8],\n         ...,\n         [0.8, 0.8, 0.8, ..., 0.8, 0.8, 0.8],\n         [0.8, 0.8, 0.8, ..., 0.8, 0.8, 0.8],\n         [0.8, 0.8, 0.8, ..., 0.8, 0.8, 0.8]],\n\n        [[0.7, 0.7, 0.7, ..., 0.7, 0.7, 0.7],\n         [0.7, 0.7, 0.7, ..., 0.7, 0.7, 0.7],\n         [0.7, 0.7, 0.7, ..., 0.7, 0.7, 0.7],\n         ...,\n         [0.7, 0.7, 0.7, ..., 0.7, 0.7, 0.7],\n         [0.7, 0.7, 0.7, ..., 0.7, 0.7, 0.7],\n         [0.7, 0.7, 0.7, ..., 0.7, 0.7, 0.7]]]])"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z - np.expand_dims(np.array([0.1, 0.2, 0.3]), axis=[1, 2])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "from torch import package\n",
    "\n",
    "path = '/home/taylor/PycharmProjects/hakai-ml-train/checkpoints/mussels/lraspp_mussels.pt'\n",
    "package_name = 'lraspp_mussels'\n",
    "resource_name = \"model.pkl\"\n",
    "\n",
    "with package.PackageExporter(path) as exp:\n",
    "    exp.intern([\"lit_lraspp_mobilenet_v3_large\", \"utils.**\", \"kelp_data_module\",\n",
    "                \"geotiff_crop_dataset.**\"])\n",
    "    exp.extern(\n",
    "        [\"pytorch_lightning.**\", \"torchmetrics.**\", \"torchvision.**\", \"numpy\", \"PIL.**\", \"tqdm.**\",\n",
    "         \"rasterio\"])\n",
    "    exp.save_pickle(package_name, resource_name, model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}