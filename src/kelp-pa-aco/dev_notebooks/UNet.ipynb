{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ceda5439-a94e-4661-962d-d6518a392f77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/data/Taylor/notebooks/KOM-kelp-aco\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "print(Path.cwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bff596f-4875-4a35-9c95-201f3c442f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_NOTEBOOK_NAME=\"/mnt/data/Taylor/notebooks/KOM-kelp-aco/UNet.ipynb\"\n"
     ]
    }
   ],
   "source": [
    "%set_env WANDB_NOTEBOOK_NAME \"/mnt/data/Taylor/notebooks/KOM-kelp-aco/UNet.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fbe66ac-0f3e-46ef-98bc-87afb01c4d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from argparse import ArgumentParser\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Union, Any\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torchmetrics.functional as fm\n",
    "import wandb\n",
    "from PIL import Image\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchmetrics.classification import Dice\n",
    "from torchvision import transforms as t\n",
    "from torchvision.transforms.functional import pad, pil_to_tensor\n",
    "from wandb import AlertLevel\n",
    "import torch.nn.functional as F\n",
    "import segmentation_models_pytorch as smp\n",
    "from lightning.pytorch.profilers import SimpleProfiler\n",
    "\n",
    "from einops import rearrange \n",
    "from unified_focal_loss import AsymmetricUnifiedFocalLoss, FocalTverskyLoss\n",
    "from datamodule import DataModule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f787f52-20c5-4dc6-b2dd-647e125d8245",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d75d130-973a-47bd-8159-4b23589495f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECKPOINT OPTIONS\n",
    "CHECKPOINT_DIR = \"./checkpoints\"\n",
    "NAME = \"UNet\"\n",
    "PROJECT_NAME = \"kom-kelp-pa-aco-rgbi\"\n",
    "\n",
    "# DATASET OPTIONS\n",
    "DATA_DIR = \"/home/taylor/data/KP-ACO-RGBI-Nov2023/\"\n",
    "NUM_WORKERS = os.cpu_count() // 2\n",
    "PIN_MEMORY = True\n",
    "NUM_CLASSES = 3\n",
    "IGNORE_INDEX = 2\n",
    "BATCH_SIZE = 2\n",
    "FILL_VALUE = 0\n",
    "NUM_BANDS = 4\n",
    "\n",
    "# MODEL OPTIONS\n",
    "LR = 0.0003\n",
    "ALPHA = 0.8\n",
    "GAMMA = 0.5\n",
    "WEIGHT_DECAY = 0.0001\n",
    "MAX_EPOCHS = 10\n",
    "PRECISION = \"16-mixed\"\n",
    "SYNC_BATCHNORM = True\n",
    "IMG_SHAPE = 1024\n",
    "DROPOUT=0.5\n",
    "WARMUP_PERIOD=1./MAX_EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fc852ec-ed9d-469e-9ffe-afc0a0da9e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.seed_everything(0, workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edcce279-d91c-49a5-93bb-6e01c739f594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make checkpoint directory\n",
    "Path(CHECKPOINT_DIR, NAME).mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11751207-5ded-4c09-98b3-e8d9147d5092",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d770b3a-75f7-4089-a2ca-0b2b5ba3b508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created by: Taylor Denouden\n",
    "# Organization: Hakai Institute\n",
    "\n",
    "class UNet(pl.LightningModule):\n",
    "    def __init__(self, num_classes: int = 2, ignore_index: Optional[int] = None, lr: float = 0.35,\n",
    "                 weight_decay: float = 0, loss_delta: float = 0.7, loss_gamma: float = 4.0 / 3.0, max_epochs: int = 100, \n",
    "                 warmup_period:float=0.3):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.ignore_index = ignore_index\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.max_epochs = max_epochs\n",
    "        self.warmup_period = warmup_period\n",
    "        \n",
    "        self.loss_delta = loss_delta\n",
    "        if self.ignore_index is not None:\n",
    "            self.n = num_classes - 1\n",
    "        else:\n",
    "            self.n = num_classes\n",
    "        \n",
    "        self.model = smp.UnetPlusPlus('resnet50', in_channels=NUM_BANDS, classes=self.n, \n",
    "                                      decoder_attention_type=\"scse\")\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = True\n",
    "        self.model = torch.compile(self.model, fullgraph=False, mode=\"max-autotune\")\n",
    "        self.loss_fn = AsymmetricUnifiedFocalLoss(delta=loss_delta, gamma=loss_gamma)\n",
    "        # self.loss_fn = FocalTverskyLoss(delta=loss_delta, gamma=loss_gamma)\n",
    "\n",
    "    @property\n",
    "    def example_input_array(self) -> Any:\n",
    "        return torch.ones((BATCH_SIZE, NUM_BANDS, IMG_SHAPE, IMG_SHAPE), device=self.device, dtype=self.dtype)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model.forward(x)\n",
    "\n",
    "    def remove_ignore_pixels(self, logits, y):\n",
    "        mask = (y != self.ignore_index)\n",
    "        return logits[mask], y[mask]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self._phase_step(batch, batch_idx, phase=\"train\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self._phase_step(batch, batch_idx, phase=\"val\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self._phase_step(batch, batch_idx, phase=\"test\")\n",
    "\n",
    "    def _phase_step(self, batch, batch_idx, phase):\n",
    "        x, y = batch\n",
    "        \n",
    "        logits = self.forward(x)\n",
    "\n",
    "        has_wandb_logger = isinstance(trainer.logger, pl.loggers.WandbLogger)\n",
    "        if phase == \"val\" and batch_idx == 0 and has_wandb_logger:\n",
    "            class_labels = {\n",
    "                0: \"background\",\n",
    "                1: \"kelp\"\n",
    "            }\n",
    "            images = [im for im in x[:,:3,:,:]]\n",
    "            masks = [\n",
    "                {\n",
    "                  \"predictions\": {\n",
    "                    \"mask_data\": pr.detach().cpu().numpy(),\n",
    "                    \"class_labels\": class_labels\n",
    "                  },\n",
    "                  \"ground_truth\": {\n",
    "                    \"mask_data\": gt.detach().cpu().numpy(),\n",
    "                    \"class_labels\": class_labels\n",
    "                  },\n",
    "                } for gt, pr in zip(y, logits.argmax(dim=1))\n",
    "            ]\n",
    "            self.logger.log_image(\"Predictions\", images=images, masks=masks)\n",
    "        \n",
    "        # Flatten and eliminate ignore class instances\n",
    "        y = rearrange(y, 'b h w -> (b h w)').long()\n",
    "        logits = rearrange(logits, 'b c h w -> (b h w) c')\n",
    "        if self.ignore_index is not None:\n",
    "            logits, y = self.remove_ignore_pixels(logits, y)\n",
    "        \n",
    "        if len(y) == 0:\n",
    "            print(\"0 length y!\")\n",
    "            return 0\n",
    "        \n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        \n",
    "        loss = self.loss_fn(probs, y.long())\n",
    "\n",
    "        accuracy = fm.accuracy(probs, y, task=\"multiclass\", num_classes=self.n, average='macro')\n",
    "        miou = fm.jaccard_index(probs, y, task=\"multiclass\", num_classes=self.n, average='macro')\n",
    "        ious = fm.jaccard_index(probs, y, task=\"multiclass\", num_classes=self.n, average='none')\n",
    "        recalls = fm.recall(probs, y, task=\"multiclass\", num_classes=self.n, average='none')\n",
    "        precisions = fm.precision(probs, y, task=\"multiclass\", num_classes=self.n, average='none')\n",
    "        f1s = fm.f1_score(probs, y, task=\"multiclass\", num_classes=self.n, average='none')\n",
    "\n",
    "        is_training = phase == \"train\"\n",
    "        self.log(f\"{phase}/loss\", loss, prog_bar=is_training)\n",
    "        self.log(f\"{phase}/miou\", miou, prog_bar=is_training)\n",
    "        self.log(f\"{phase}/accuracy\", accuracy)\n",
    "\n",
    "        for c in range(self.n):\n",
    "            clsx = f\"cls{c + 1}\" if self.ignore_index and c >= self.ignore_index else f\"cls{c}\"\n",
    "            self.log(f\"{phase}/{clsx}_iou\", ious[c], prog_bar=is_training)\n",
    "            self.log_dict({\n",
    "                f\"{phase}/{clsx}_recall\": recalls[c],\n",
    "                f\"{phase}/{clsx}_precision\": precisions[c],\n",
    "                f\"{phase}/{clsx}_f1\": f1s[c],\n",
    "            })\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Init optimizer and scheduler\"\"\"\n",
    "        optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, self.parameters()),\n",
    "                                      lr=self.lr, weight_decay=self.weight_decay, amsgrad=True)\n",
    "        \n",
    "        steps = self.trainer.estimated_stepping_batches\n",
    "        warmup_steps = steps*self.warmup_period\n",
    "        \n",
    "        linear_warmup_sch = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=0.001, total_iters=warmup_steps)\n",
    "        cosine_sch = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=(steps-warmup_steps))\n",
    "        lr_scheduler = torch.optim.lr_scheduler.SequentialLR(optimizer, [linear_warmup_sch, cosine_sch], milestones=[warmup_steps])\n",
    "        \n",
    "        return [optimizer], [{\"scheduler\": lr_scheduler, \"interval\": \"step\"}]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a79524-53d2-459e-ac5c-1bf12473a18c",
   "metadata": {},
   "source": [
    "# Setup Callbacks and Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56587af0-e811-4ea9-b3d9-721cfbf6bcfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/taylor/miniforge3/envs/kom/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:668: You passed `deterministic=True` and `benchmark=True`. Note that PyTorch ignores torch.backends.cudnn.deterministic=True when torch.backends.cudnn.benchmark=True.\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Running in `fast_dev_run` mode: will run the requested loop using 1 batch(es). Logging and checkpointing is suppressed.\n"
     ]
    }
   ],
   "source": [
    "ENABLE_LOGGING = True\n",
    "\n",
    "profiler = SimpleProfiler(dirpath=\".\", filename=\"perf_logs\")\n",
    "\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    monitor=\"val/miou\",\n",
    "    mode=\"max\",\n",
    "    filename=\"{val/miou:.4f}_{epoch}\",\n",
    "    save_top_k=1,\n",
    "    save_last=True,\n",
    "    save_on_train_epoch_end=False,\n",
    "    every_n_epochs=1,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "if ENABLE_LOGGING:\n",
    "    logger = WandbLogger(\n",
    "        name=NAME,\n",
    "        project=PROJECT_NAME,\n",
    "        save_dir=CHECKPOINT_DIR,\n",
    "        log_model=True,\n",
    "    )\n",
    "    logger.experiment.config[\"batch_size\"] = BATCH_SIZE\n",
    "    # logger.experiment.config[\"dropout\"] = DROPOUT\n",
    "else:\n",
    "    logger = pl.loggers.CSVLogger(save_dir=\"/tmp/\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    # profiler=profiler,\n",
    "    # overfit_batches=10,\n",
    "    # log_every_n_steps=3,\n",
    "    # limit_train_batches=3,\n",
    "    # limit_val_batches=3,\n",
    "    # accelerator='cpu',\n",
    "    # fast_dev_run=True,\n",
    "    deterministic=True,\n",
    "    benchmark=True,\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    precision=PRECISION,\n",
    "    logger=logger,\n",
    "    gradient_clip_val=0.5,\n",
    "    accumulate_grad_batches=8,\n",
    "    callbacks=[\n",
    "        checkpoint_callback,\n",
    "        pl.callbacks.LearningRateMonitor(),\n",
    "#         Finetuning(unfreeze_at_epoch=FINETUNE_EPOCH)\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c974b14-1ae3-42c3-9d10-f973434e6dc3",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7789d51d-0a21-4e55-bf8b-cda0d978c2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = DataModule(\n",
    "    DATA_DIR,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY,\n",
    "    persistent_workers=True,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    fill_value=FILL_VALUE,\n",
    "    tile_size=IMG_SHAPE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71840b0b-2b39-41f6-974d-1bbd992c40c3",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f759cd7-348c-4ad4-a942-196a2ef9507e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the model directly on the device and with parameters in half-precision\n",
    "model = UNet(\n",
    "    num_classes=NUM_CLASSES,\n",
    "    ignore_index=IGNORE_INDEX,\n",
    "    lr=LR,\n",
    "    loss_delta=ALPHA,\n",
    "    loss_gamma=GAMMA,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    # dropout=DROPOUT,\n",
    "    warmup_period=WARMUP_PERIOD,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ec15f2-193f-4d04-91ec-e3b447a5f45e",
   "metadata": {},
   "source": [
    "# TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "009ac9be-3927-4adb-ac0c-04ce13f1c5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/taylor/miniforge3/envs/kom/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:634: Checkpoint directory /mnt/data/Taylor/notebooks/KOM-kelp-aco/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\n",
      "  | Name    | Type                       | Params | In sizes | Out sizes\n",
      "------------------------------------------------------------------------------\n",
      "0 | model   | OptimizedModule            | 51.1 M | ?        | ?        \n",
      "1 | loss_fn | AsymmetricUnifiedFocalLoss | 0      | ?        | ?        \n",
      "------------------------------------------------------------------------------\n",
      "51.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "51.1 M    Total params\n",
      "204.515   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fd03f19b71d468c88c3c4125b22db44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=1` reached.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/tmp/lightning_logs/version_2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m     wandb\u001b[38;5;241m.\u001b[39mfinish()\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 21\u001b[0m     \u001b[43mshutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrmtree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/kom/lib/python3.11/shutil.py:722\u001b[0m, in \u001b[0;36mrmtree\u001b[0;34m(path, ignore_errors, onerror, dir_fd)\u001b[0m\n\u001b[1;32m    720\u001b[0m     orig_st \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mlstat(path, dir_fd\u001b[38;5;241m=\u001b[39mdir_fd)\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m--> 722\u001b[0m     \u001b[43monerror\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    723\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/kom/lib/python3.11/shutil.py:720\u001b[0m, in \u001b[0;36mrmtree\u001b[0;34m(path, ignore_errors, onerror, dir_fd)\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[38;5;66;03m# Note: To guard against symlink races, we use the standard\u001b[39;00m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;66;03m# lstat()/open()/fstat() trick.\u001b[39;00m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 720\u001b[0m     orig_st \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mlstat(path, dir_fd\u001b[38;5;241m=\u001b[39mdir_fd)\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    722\u001b[0m     onerror(os\u001b[38;5;241m.\u001b[39mlstat, path, sys\u001b[38;5;241m.\u001b[39mexc_info())\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/tmp/lightning_logs/version_2'"
     ]
    }
   ],
   "source": [
    "# %%debug\n",
    "\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "\n",
    "try:\n",
    "    trainer.fit(model, datamodule=data_module)\n",
    "    \n",
    "    if not trainer.fast_dev_run and ENABLE_LOGGING:\n",
    "        best_miou = checkpoint_callback.best_model_score.detach().cpu()\n",
    "        print(\"Best mIoU:\", best_miou)\n",
    "        wandb.alert(\n",
    "            title=\"Training complete\",\n",
    "            text=f\"Best mIoU: {best_miou}\",\n",
    "            level=AlertLevel.INFO,\n",
    "        )\n",
    "\n",
    "finally:\n",
    "    if ENABLE_LOGGING:\n",
    "        wandb.finish()\n",
    "    else:\n",
    "        shutil.rmtree(logger.log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca1f11ec-7fb5-40be-be32-a6389e2975db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook UNet.ipynb to script\n",
      "[NbConvertApp] Writing 9921 bytes to UNet.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script \"UNet.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ce7239-0ede-44e4-9020-272b75e87d3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
