{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "name": ""
    }
   },
   "source": [
    "Current Kelp Presence weights:\n",
    "\"checkpoints/kelp_pa/LRASPP/Feb2022/best-val_miou=0.8023-epoch=18-step=17593.pt\"\n",
    "\n",
    "Current Kelp Species weights:\n",
    "\"checkpoints/kelp_species/checkpoints/checkpoints/kelp_species_lraspp/version_2/checkpoints/val/miou=0.9634_epoch=13.pt\"\n",
    "\n",
    "Current Kelp Mussels weights:\n",
    "\"checkpoints/mussels/LRASPP/Aug2022/val_miou=0.8384_epoch=4.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/taylor/PycharmProjects/hakai-ml-train/src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "import onnx\n",
    "from torchvision import transforms as t\n",
    "from utils.transforms import PadOut, normalize, target_to_tensor\n",
    "from PIL import Image\n",
    "import onnxruntime\n",
    "import numpy as np\n",
    "\n",
    "from models.lit_lraspp_mobilenet_v3_large import LRASPPMobileNetV3Large\n",
    "DEVICE = torch.device('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(1, 3, 8, 8, device=DEVICE, requires_grad=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Presence model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_weights_path = \"/home/taylor/PycharmProjects/hakai-ml-train/exports/presence/best-val_miou=0.8023-epoch=18-step=17593_fix.ckpt\"\n",
    "\n",
    "p_model = LRASPPMobileNetV3Large.load_from_checkpoint(p_weights_path, train=False, map_location=DEVICE, strict=False, num_classes=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export Torch JIT Trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_torchscript_path = \"/home/taylor/PycharmProjects/hakai-ml-train/exports/presence/LRASPP_MobileNetV3_kelp_presence_jit_miou=0.8023.pt\"\n",
    "_ = p_model.to_torchscript(file_path=p_torchscript_path, method='trace', example_inputs=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export ONNX\n",
    "\n",
    "I've contructed a new model that takes care of the data normalization and argmax on the logits, with the intention being to eliminate the need for a pytorch installation on user's machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Diagnostic Run torch.onnx.export version 2.0.0 ================\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p_onnx_path = \"/home/taylor/PycharmProjects/hakai-ml-train/exports/presence/LRASPP_MobileNetV3_kelp_presence_miou=0.8023.onnx\"\n",
    "\n",
    "\n",
    "class PresenceInferenceModel(LRASPPMobileNetV3Large):\n",
    "    def forward(self, x):\n",
    "        logits = super().forward(normalize(x))\n",
    "        return torch.argmax(logits, dim=1)\n",
    "        \n",
    "p_inf_model = PresenceInferenceModel.load_from_checkpoint(p_weights_path, train=False, map_location=DEVICE, strict=False, num_classes=2)\n",
    "    \n",
    "p_inf_model.to_onnx(\n",
    "    p_onnx_path, \n",
    "    x, \n",
    "    export_params=True, \n",
    "    input_names=[\"x\"],\n",
    "    output_names=[\"pred\"],\n",
    "    dynamic_axes={\n",
    "        \"x\": {\n",
    "            0: \"batch_size\", \n",
    "            2: \"height\", \n",
    "            3: \"width\"\n",
    "        },\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test the exports all do what they're supposed to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = t.Compose([PadOut(512, 512, fill_value=0), t.ToTensor(), normalize])\n",
    "# No normalization\n",
    "trans1 = t.Compose([PadOut(512, 512, fill_value=0), t.ToTensor()])\n",
    "img = Image.open(\"/home/taylor/PycharmProjects/hakai-ml-train/exports/Simmonds_kelp_U0171_08_03.tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512, 512])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = trans(img).repeat(repeats=[2,1,1,1]).to(DEVICE)\n",
    "#Unnormalized\n",
    "x1 = trans1(img).repeat(repeats=[2,1,1,1]).to(DEVICE)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_model_kom = torch.jit.load(\"/home/taylor/PycharmProjects/hakai-ml-train/exports/LRASPP_MobileNetV3_kelp_presence_jit.pt\", map_location=DEVICE)\n",
    "p_model_kom = p_model_kom.eval()\n",
    "\n",
    "# KoM with jit weights output\n",
    "with torch.no_grad():\n",
    "    p_kom_out = p_model_kom(x).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_model_test = torch.jit.load(p_torchscript_path, map_location=DEVICE)\n",
    "p_model_test = p_model_test.eval()\n",
    "\n",
    "# Test that new JIT export matches KOM outputs\n",
    "with torch.no_grad():\n",
    "    p_test_out = p_model_test(x).numpy()\n",
    "    \n",
    "np.allclose(p_kom_out, p_test_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ort_session = onnxruntime.InferenceSession(p_onnx_path)\n",
    "input_name = ort_session.get_inputs()[0].name\n",
    "ort_inputs = {input_name: x1.numpy()}\n",
    "p_ort_outs = ort_session.run(None, ort_inputs)\n",
    "\n",
    "# Test that onnx output matches KOM output\n",
    "np.allclose(np.argmax(p_kom_out, axis=1), p_ort_outs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Species model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_weights_path = \"/home/taylor/PycharmProjects/hakai-ml-train/exports/species/miou=0.9634_epoch=13.ckpt\"\n",
    "s_model = LRASPPMobileNetV3Large.load_from_checkpoint(s_weights_path, train=False, map_location=DEVICE, strict=False, num_classes=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export JIT Trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_torchscript_path = \"/home/taylor/PycharmProjects/hakai-ml-train/exports/species/LRASPP_MobileNetV3_kelp_species_jit_miou=0.9634.pt\"\n",
    "_ = s_model.to_torchscript(file_path=s_torchscript_path, method='trace', example_inputs=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Diagnostic Run torch.onnx.export version 2.0.0 ================\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s_onnx_path = \"/home/taylor/PycharmProjects/hakai-ml-train/exports/species/LRASPP_MobileNetV3_kelp_species_miou=0.9634.onnx\"\n",
    "\n",
    "class SpeciesInferenceModel(LRASPPMobileNetV3Large):\n",
    "    def update_presence_model(self, model):\n",
    "        # PresenceInferenceModel\n",
    "        self.presence_model = model\n",
    "        \n",
    "    def forward(self, x):\n",
    "        presence = self.presence_model.forward(x)  # 0: bg, 1: kelp\n",
    "        \n",
    "        s_logits = super().forward(normalize(x))\n",
    "        species = torch.add(torch.argmax(s_logits, dim=1), 2)  # 2: macro, 3: nereo\n",
    "\n",
    "        return torch.mul(presence, species)  # 0: bg, 2: macro, 3: nereo\n",
    "\n",
    "        \n",
    "s_inf_model = SpeciesInferenceModel.load_from_checkpoint(s_weights_path, train=False, map_location=DEVICE, strict=False, num_classes=2)\n",
    "s_inf_model.update_presence_model(p_inf_model)\n",
    "\n",
    "s_inf_model.to_onnx(\n",
    "    s_onnx_path, \n",
    "    x, \n",
    "    export_params=True, \n",
    "    input_names=[\"x\"],\n",
    "    output_names=[\"pred\"],\n",
    "    dynamic_axes={\n",
    "        \"x\": {\n",
    "            0: \"batch_size\", \n",
    "            2: \"height\", \n",
    "            3: \"width\"\n",
    "        },\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test the exports all do what they're supposed to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_model_kom = torch.jit.load(\"/home/taylor/PycharmProjects/hakai-ml-train/exports/LRASPP_MobileNetV3_kelp_species_jit_miou=0.9634.pt\", map_location=DEVICE)\n",
    "s_model_kom = s_model_kom.eval()\n",
    "\n",
    "# KoM with jit weights output\n",
    "with torch.no_grad():\n",
    "    s_kom_out = s_model_kom(x).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_model_test = torch.jit.load(s_torchscript_path, map_location=DEVICE)\n",
    "s_model_test = s_model_test.eval()\n",
    "\n",
    "# Test that new JIT export matches KOM outputs\n",
    "with torch.no_grad():\n",
    "    s_test_out = s_model_test(x).numpy()\n",
    "    \n",
    "np.allclose(s_kom_out, s_test_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ort_session = onnxruntime.InferenceSession(s_onnx_path)\n",
    "input_name = ort_session.get_inputs()[0].name\n",
    "ort_inputs = {input_name: x1.numpy()}\n",
    "s_ort_outs = ort_session.run(None, ort_inputs)\n",
    "\n",
    "# Test that onnx output matches KOM output\n",
    "np.allclose(\n",
    "    np.multiply(np.argmax(p_kom_out, axis=1), np.argmax(s_kom_out, axis=1) + 2),\n",
    "    s_ort_outs[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mussels model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_weights_path = \"/home/taylor/PycharmProjects/hakai-ml-train/exports/mussels/val_miou=0.8384_epoch=4.ckpt\"\n",
    "\n",
    "m_model = LRASPPMobileNetV3Large.load_from_checkpoint(m_weights_path, train=False, map_location=DEVICE, strict=False, num_classes=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export Torch JIT Trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_torchscript_path = \"/home/taylor/PycharmProjects/hakai-ml-train/exports/mussels/LRASPP_MobileNetV3_mussel_presence_jit_miou=0.8384.pt\"\n",
    "_ = m_model.to_torchscript(file_path=m_torchscript_path, method='trace', example_inputs=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export ONNX\n",
    "\n",
    "I've contructed a new model that takes care of the data normalization and argmax on the logits, with the intention being to eliminate the need for a pytorch installation on user's machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Diagnostic Run torch.onnx.export version 2.0.0 ================\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "m_onnx_path = \"/home/taylor/PycharmProjects/hakai-ml-train/exports/mussels/LRASPP_MobileNetV3_mussel_presence_miou=0.8384.onnx\"\n",
    "\n",
    "\n",
    "class MusselsInferenceModel(LRASPPMobileNetV3Large):\n",
    "    def forward(self, x):\n",
    "        logits = super().forward(normalize(x))\n",
    "        return torch.argmax(logits, dim=1)\n",
    "        \n",
    "m_inf_model = MusselsInferenceModel.load_from_checkpoint(m_weights_path, train=False, map_location=DEVICE, strict=False, num_classes=2)\n",
    "    \n",
    "m_inf_model.to_onnx(\n",
    "    m_onnx_path, \n",
    "    x, \n",
    "    export_params=True, \n",
    "    input_names=[\"x\"],\n",
    "    output_names=[\"pred\"],\n",
    "    dynamic_axes={\n",
    "        \"x\": {\n",
    "            0: \"batch_size\", \n",
    "            2: \"height\", \n",
    "            3: \"width\"\n",
    "        },\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test the exports all do what they're supposed to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = t.Compose([PadOut(512, 512, fill_value=0), t.ToTensor(), normalize])\n",
    "# No normalization\n",
    "trans1 = t.Compose([PadOut(512, 512, fill_value=0), t.ToTensor()])\n",
    "img = Image.open(\"/home/taylor/PycharmProjects/hakai-ml-train/exports/Simmonds_kelp_U0171_08_03.tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512, 512])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = trans(img).repeat(repeats=[2,1,1,1]).to(DEVICE)\n",
    "#Unnormalized\n",
    "x1 = trans1(img).repeat(repeats=[2,1,1,1]).to(DEVICE)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_model_kom = torch.jit.load(\"/home/taylor/PycharmProjects/hakai-ml-train/exports/LRASPP_MobileNetV3_mussel_presence_jit_v2.pt\", map_location=DEVICE)\n",
    "m_model_kom = m_model_kom.eval()\n",
    "\n",
    "# KoM with jit weights output\n",
    "with torch.no_grad():\n",
    "    m_kom_out = m_model_kom(x).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_model_test = torch.jit.load(m_torchscript_path, map_location=DEVICE)\n",
    "m_model_test = m_model_test.eval()\n",
    "\n",
    "# Test that new JIT export matches KOM outputs\n",
    "with torch.no_grad():\n",
    "    m_test_out = m_model_test(x).numpy()\n",
    "    \n",
    "np.allclose(m_kom_out, m_test_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ort_session = onnxruntime.InferenceSession(m_onnx_path)\n",
    "input_name = ort_session.get_inputs()[0].name\n",
    "ort_inputs = {input_name: x1.numpy()}\n",
    "m_ort_outs = ort_session.run(None, ort_inputs)\n",
    "\n",
    "# Test that onnx output matches KOM output\n",
    "np.allclose(np.argmax(m_kom_out, axis=1), m_ort_outs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
